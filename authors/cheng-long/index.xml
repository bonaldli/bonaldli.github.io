<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cheng Long | Bonald Ziyue LI</title>
    <link>https://bonaldli.github.io/authors/cheng-long/</link>
      <atom:link href="https://bonaldli.github.io/authors/cheng-long/index.xml" rel="self" type="application/rss+xml" />
    <description>Cheng Long</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 07 Jun 2024 14:58:48 +0200</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Cheng Long</title>
      <link>https://bonaldli.github.io/authors/cheng-long/</link>
    </image>
    
    <item>
      <title>Spatial-Temporal Large Language Model for Traffic Prediction</title>
      <link>https://bonaldli.github.io/publication/stllm/</link>
      <pubDate>Fri, 07 Jun 2024 14:58:48 +0200</pubDate>
      <guid>https://bonaldli.github.io/publication/stllm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>TimeCMA: Towards LLM-Empowered Time Series Forecasting via Cross-Modality Alignment</title>
      <link>https://bonaldli.github.io/publication/timecma/</link>
      <pubDate>Mon, 03 Jun 2024 15:01:39 +0200</pubDate>
      <guid>https://bonaldli.github.io/publication/timecma/</guid>
      <description></description>
    </item>
    
    <item>
      <title>FELight: Fairness-Aware Traffic Signal Control Via Sample-Efficient Reinforcement Learning</title>
      <link>https://bonaldli.github.io/publication/felight/</link>
      <pubDate>Mon, 18 Mar 2024 14:59:50 +0200</pubDate>
      <guid>https://bonaldli.github.io/publication/felight/</guid>
      <description>&lt;p&gt;&lt;strong&gt;TL; DR&lt;/strong&gt;:  It&amp;rsquo;s traffic signal control again, but what&amp;rsquo;s new here? How can we ensure that the agents control the traffic light fairly? We embedded such fairness metrics into Reinforcement Learning models. Furthermore, how can we minimize the sample required and achieve a resource-efficient agent? We used self-supervised learning!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>KITS: Inductive Spatio-Temporal Kriging with Increment Training Strategy</title>
      <link>https://bonaldli.github.io/publication/kits-iclr24/</link>
      <pubDate>Sun, 05 Nov 2023 00:06:12 +0100</pubDate>
      <guid>https://bonaldli.github.io/publication/kits-iclr24/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
